{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invasive Species\n",
    "\n",
    "## Caso 1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importacion de Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de Datos\n",
    "\n",
    "Se usará un generador para la carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Se genera un dataframe con la ruta a las imagenes de entrenamiento y su categoria: 0 No invasora, 1 Invasora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>../Data/train/1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>../Data/train/2.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>../Data/train/3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>../Data/train/4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>../Data/train/5.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  invasive\n",
       "0  ../Data/train/1.jpg         0\n",
       "1  ../Data/train/2.jpg         0\n",
       "2  ../Data/train/3.jpg         1\n",
       "3  ../Data/train/4.jpg         0\n",
       "4  ../Data/train/5.jpg         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se cargan los path a los archivos de entrenamiento\n",
    "path_images = \"../Data/train/\"\n",
    "path_labels = \"../Data/train_labels.csv\"\n",
    "\n",
    "#se define una funcion para cargar los datos\n",
    "def load_train(path_images, path_labels):\n",
    "    #path_images: carpeta donde estan las imagenes de entreniento\n",
    "    #path_labels: ubicacion del archivo csv con dos columnas: name - invasive (binario)\n",
    "    train_set = pd.read_csv(path_labels)  #se carga el archivo csv con pandas\n",
    "    train_label = np.array(train_set['invasive'].iloc[: ])  #se transforma a numpy array\n",
    "    train_files = []\n",
    "    for i in range(len(train_set)):\n",
    "        train_files.append(path_images + str(int(train_set.iloc[i][0])) +'.jpg')\n",
    "    train_set['name'] = train_files\n",
    "    return train_files, train_set, train_label\n",
    "\n",
    "train_files, train_set, train_label = load_train(path_images,path_labels)\n",
    "\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Data/train/1.jpg',\n",
       " '../Data/train/2.jpg',\n",
       " '../Data/train/3.jpg',\n",
       " '../Data/train/4.jpg',\n",
       " '../Data/train/5.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Se cargan los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  invasive\n",
       "0     1       0.5\n",
       "1     2       0.5\n",
       "2     3       0.5\n",
       "3     4       0.5\n",
       "4     5       0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../input/test/\"\n",
    "\n",
    "def load_test(path):\n",
    "    test_set = pd.read_csv('../Data/sample_submission.csv')\n",
    "    test_files = []\n",
    "    for i in range(len(test_set)):\n",
    "        test_files.append(path + str(int(test_set.iloc[i][0])) +'.jpg')\n",
    "    return test_files, test_set\n",
    "\n",
    "test_files, test_set = load_test(path)\n",
    "\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del Modelo\n",
    "\n",
    "Se usa transfer learning con un modelo pre entrenado Inception V3 que fue creado por Google en 2015 con un entrenamiento sobre 1.000 clases y entrenado sobre un millón de imágenes.\n",
    "Keras tiene un modelo inceptionV3 incorporado cuya entrada por defecto es 299x299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 800, 800, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 800, 800, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 23, 23, 2048)      21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 21,804,845\n",
      "Trainable params: 21,770,407\n",
      "Non-trainable params: 34,438\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_height = 800\n",
    "img_width = 800\n",
    "img_channels = 3\n",
    "img_dim = (img_height, img_width, img_channels)\n",
    "\n",
    "#función para defenir el modelo. No es necesario pero es una buena práctica\n",
    "def inceptionv3(img_dim=img_dim):\n",
    "    input_tensor = Input(shape=img_dim)  #definición del input con las dimensiones de las imagenes\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=img_dim)  #se carga el modelo pre entrenado inceptionV3 con los weigths de imagenet, se expluye la capa final\n",
    "    bn = BatchNormalization()(input_tensor)  #aquí se hace el re escalado en lugar de las imagenes\n",
    "    x = base_model(bn)  #se incorporan las imagenes normalizadas a modelo inception\n",
    "    x = GlobalAveragePooling2D()(x)  #transforma los tensores en un vector\n",
    "    x = Dropout(0.5)(x)  #capa droput para evitar overfitting\n",
    "    output = Dense(1, activation='sigmoid')(x)  #se define la salida que es binaria según el problema a resolver\n",
    "    model = Model(input_tensor, output)  #paso final en la definción del modelo en Keras\n",
    "    return model\n",
    "\n",
    "model = inceptionv3()  #Definición del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, epochs, img_size, x, y, test, n_fold, kf):\n",
    "    roc_auc = metrics.roc_auc_score\n",
    "    preds_train = np.zeros(len(x), dtype = np.float)\n",
    "    preds_test = np.zeros(len(test), dtype = np.float)\n",
    "    train_scores = []; valid_scores = []\n",
    "\n",
    "    i = 1\n",
    "\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train = x.iloc[train_index]; x_valid = x.iloc[test_index]\n",
    "        y_train = y[train_index]; y_valid = y[test_index]\n",
    "        #se realiza Data Augmentation en el generador de datos - es más eficiente\n",
    "        def augment(src, choice):\n",
    "            if choice == 0:\n",
    "                # Rotate 90\n",
    "                src = np.rot90(src, 1)\n",
    "            if choice == 1:\n",
    "                # flip vertically\n",
    "                src = np.flipud(src)\n",
    "            if choice == 2:\n",
    "                # Rotate 180\n",
    "                src = np.rot90(src, 2)\n",
    "            if choice == 3:\n",
    "                # flip horizontally\n",
    "                src = np.fliplr(src)\n",
    "            if choice == 4:\n",
    "                # Rotate 90 counter-clockwise\n",
    "                src = np.rot90(src, 3)\n",
    "            if choice == 5:\n",
    "                # Rotate 180 and flip horizontally\n",
    "                src = np.rot90(src, 2)\n",
    "                src = np.fliplr(src)\n",
    "            return src\n",
    "        #se define el generador de datos para que el modelo tome imagenes de manera continua según la cantidad de datos\n",
    "        #el keras ya lo trae incorporado pero aquí se define manualmente para tener certeza del data augmentation\n",
    "        def train_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(x_train), batch_size):\n",
    "                    x_batch = []\n",
    "                    y_batch = []\n",
    "                    end = min(start + batch_size, len(x_train))\n",
    "                    train_batch = x_train[start:end]\n",
    "                    for filepath, tag in train_batch.values:\n",
    "                        img = cv2.imread(filepath)\n",
    "                        img = cv2.resize(img, img_size)\n",
    "                        img = augment(img, np.random.randint(6))\n",
    "                        x_batch.append(img)\n",
    "                        y_batch.append(tag)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    y_batch = np.array(y_batch, np.uint8)\n",
    "                    yield x_batch, y_batch\n",
    "\n",
    "        def valid_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(x_valid), batch_size):\n",
    "                    x_batch = []\n",
    "                    y_batch = []\n",
    "                    end = min(start + batch_size, len(x_valid))\n",
    "                    valid_batch = x_valid[start:end]\n",
    "                    for filepath, tag in valid_batch.values:\n",
    "                        img = cv2.imread(filepath)\n",
    "                        img = cv2.resize(img, img_size)\n",
    "                        img = augment(img, np.random.randint(6))\n",
    "                        x_batch.append(img)\n",
    "                        y_batch.append(tag)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    y_batch = np.array(y_batch, np.uint8)\n",
    "                    yield x_batch, y_batch\n",
    "\n",
    "        def test_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(test), batch_size):\n",
    "                    x_batch = []\n",
    "                    end = min(start + batch_size, len(test))\n",
    "                    test_batch = test[start:end]\n",
    "                    for filepath in test_batch:\n",
    "                        img = cv2.imread(filepath)\n",
    "                        img = cv2.resize(img, img_size)\n",
    "                        x_batch.append(img)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    yield x_batch\n",
    "\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, cooldown=1, \n",
    "                               verbose=1, min_lr=1e-7),\n",
    "             ModelCheckpoint(filepath='../Saved_Model/model' + str(i) + '.hdf5', verbose=1,\n",
    "                             save_best_only=True, save_weights_only=True, mode='auto')]\n",
    "\n",
    "        train_steps = len(x_train) / batch_size\n",
    "        valid_steps = len(x_valid) / batch_size\n",
    "        test_steps = len(test) / batch_size\n",
    "        \n",
    "        model = model\n",
    "\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', \n",
    "                      metrics = ['accuracy'])\n",
    "\n",
    "        model.fit_generator(train_generator(), train_steps, epochs=epochs, verbose=1, \n",
    "                            callbacks=callbacks, validation_data=valid_generator(), \n",
    "                            validation_steps=valid_steps)\n",
    "\n",
    "        model.load_weights(filepath='../Saved_Model/model' + str(i) + '.hdf5')\n",
    "\n",
    "        print('Running validation predictions on fold {}'.format(i))\n",
    "        preds_valid = model.predict_generator(generator=valid_generator(),\n",
    "                                      steps=valid_steps, verbose=1)[:, 0]\n",
    "\n",
    "        print('Running train predictions on fold {}'.format(i))\n",
    "        preds_train = model.predict_generator(generator=train_generator(),\n",
    "                                      steps=train_steps, verbose=1)[:, 0]\n",
    "\n",
    "        valid_score = roc_auc(y_valid, preds_valid)\n",
    "        train_score = roc_auc(y_train, preds_train)\n",
    "        print('Val Score:{} for fold {}'.format(valid_score, i))\n",
    "        print('Train Score: {} for fold {}'.format(train_score, i))\n",
    "\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        print('Avg Train Score:{0:0.5f}, Val Score:{1:0.5f} after {2:0.5f} folds'.format\n",
    "              (np.mean(train_scores), np.mean(valid_scores), i))\n",
    "\n",
    "        print('Running test predictions with fold {}'.format(i))\n",
    "\n",
    "        preds_test_fold = model.predict_generator(generator=test_generator(),\n",
    "                                              steps=test_steps, verbose=1)[:, -1]\n",
    "\n",
    "        preds_test += preds_test_fold\n",
    "\n",
    "        print('\\n\\n')\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        if i <= n_fold:\n",
    "            print('Now beginning training for fold {}\\n\\n'.format(i))\n",
    "        else:\n",
    "            print('Finished training!')\n",
    "\n",
    "    preds_test /= n_fold\n",
    "\n",
    "\n",
    "    return preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "epochs = 5\n",
    "n_fold = 2\n",
    "img_size = (img_height, img_width)\n",
    "kf = KFold(n_splits=n_fold, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Usuario\\Anaconda3\\envs\\GPU_TF2\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/5\n",
      "574/573 [==============================] - 777s 1s/step - loss: 0.3983 - accuracy: 0.8317 - val_loss: 5.3964e-11 - val_accuracy: 0.9155\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00000, saving model to ../Saved_Model/model1.hdf5\n",
      "Epoch 2/5\n",
      "574/573 [==============================] - 726s 1s/step - loss: 0.1879 - accuracy: 0.9268 - val_loss: 1.3956e-13 - val_accuracy: 0.9686\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to ../Saved_Model/model1.hdf5\n",
      "Epoch 3/5\n",
      "574/573 [==============================] - 728s 1s/step - loss: 0.0943 - accuracy: 0.9730 - val_loss: 4.3861e-06 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00000\n",
      "Epoch 4/5\n",
      "574/573 [==============================] - 728s 1s/step - loss: 0.0642 - accuracy: 0.9852 - val_loss: 5.9582e-06 - val_accuracy: 0.9756\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00000\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_weights() missing 1 required positional argument: 'filepath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-069f394825bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m test_pred = train_model(model, batch_size, epochs, img_size, train_set, \n\u001b[1;32m----> 2\u001b[1;33m                         train_label, test_files, n_fold, kf)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-9bef51ab82a5>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, batch_size, epochs, img_size, x, y, test, n_fold, kf)\u001b[0m\n\u001b[0;32m    100\u001b[0m                             validation_steps=valid_steps)\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../Saved_Model/model'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Running validation predictions on fold {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\GPU_TF2\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: load_weights() missing 1 required positional argument: 'filepath'"
     ]
    }
   ],
   "source": [
    "test_pred = train_model(model, batch_size, epochs, img_size, train_set, \n",
    "                        train_label, test_files, n_fold, kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
